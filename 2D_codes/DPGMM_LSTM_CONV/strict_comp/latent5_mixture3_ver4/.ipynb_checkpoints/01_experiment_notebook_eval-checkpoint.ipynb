{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show reconstruct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiroki/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# load config\n",
    "########################################################################\n",
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting I/O path\n",
    "########################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "eval_path = INPUT_ROOT + \"/eval_test\"\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "RESULT_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/result'\n",
    "RECONS_OUTDIR = OUTPUT_ROOT +'/eval_reconstruct_img'\n",
    "PKL_DIR = OUTPUT_ROOT +'/pkl'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default python-library\n",
    "########################################################################\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional python-library\n",
    "########################################################################\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import common as com\n",
    "import pytorch_modeler as modeler\n",
    "from pytorch_model import DAGMM as Model\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "yaml.warnings({'YAMLLoadWarning': False})\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "########################################################################\n",
    "import eval_functions as eval_func\n",
    "from pytorch_utils import to_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiroki/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# load config\n",
    "########################################################################\n",
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting seed\n",
    "########################################################################\n",
    "modeler.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting I/O path\n",
    "########################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "eval_path = INPUT_ROOT + \"/eval_test\"\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "RESULT_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/result'\n",
    "RECONS_OUTDIR = OUTPUT_ROOT +'/eval_reconstruct_img'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# for original function\n",
    "########################################################################\n",
    "param = {}\n",
    "param[\"dev_directory\"] = dev_path\n",
    "param[\"eval_directory\"] = eval_path\n",
    "param[\"model_directory\"] = MODEL_DIR\n",
    "param[\"result_directory\"] = RESULT_DIR\n",
    "param[\"result_file\"] = 'result.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def calc_time_anomaly(x, y, label, file_name):\n",
    "    fig = plt.figure(figsize=(10,5)) # width, height\n",
    "    fig.suptitle('label={}'.format(int(label)))\n",
    "    time_anomaly = np.zeros((x.shape[0]))\n",
    "    for frame in range(x.shape[0]):\n",
    "        time_anomaly[frame] = mse(y[frame,:],x[frame,:])\n",
    "    plt.plot(time_anomaly)\n",
    "    plt.title(f'label:{label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_anomaly.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(time_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(np.abs(x-y), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reconstruct_img(x, y, label, file_name):\n",
    "    fig = plt.figure(figsize=(10,5)) # width, height\n",
    "    fig.suptitle('label={}'.format(int(label)))\n",
    "    ax1 = fig.add_subplot(121, title='x') # 明示的にAxesを作成する\n",
    "    sns.heatmap(x.T, ax=ax1) # ax1を参照するようにする\n",
    "    ax2 = fig.add_subplot(122, title='y')\n",
    "    sns.heatmap(y.T, ax=ax2)\n",
    "    fig.savefig('{}.png'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import preprocessing as prep\n",
    "\n",
    "class extract_waveform(object):\n",
    "    \"\"\"\n",
    "    wavデータロード(波形)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    sound_data : waveform\n",
    "    \"\"\"\n",
    "    def __init__(self, sound_data=None):\n",
    "        self.sound_data = sound_data\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        self.sound_data = com.file_load(sample['wav_name'],\n",
    "                                        sr=config['preprocessing']['sample_rate'],\n",
    "                                        mono=config['preprocessing']['mono'])\n",
    "        self.sound_data = self.sound_data[0]\n",
    "        self.label = np.array(sample['label'])\n",
    "        self.wav_name = sample['wav_name']\n",
    "        \n",
    "        return {'feature': self.sound_data, 'label': self.label, 'wav_name': self.wav_name}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        feature, label, wav_name = sample['feature'], sample['label'], sample['wav_name']\n",
    "        \n",
    "        return {'feature': torch.from_numpy(feature).float(), 'label': torch.from_numpy(label), 'wav_name': wav_name}\n",
    "\n",
    "class DCASE_task2_Dataset_test(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Attribute\n",
    "    ----------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.transform = transform\n",
    "        self.file_list = file_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        # ファイル名でlabelを判断\n",
    "        if \"normal\" in file_path:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "        sample = {'wav_name':file_path, 'label':np.array(label)}\n",
    "        sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def make_dataloader_train(paths):\n",
    "    transform = transforms.Compose([\n",
    "        extract_waveform(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    dataset = DCASE_task2_Dataset_test(paths, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=config['fit']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return test_loader\n",
    "    \n",
    "def make_dataloader_test(paths):\n",
    "    transform = transforms.Compose([\n",
    "        extract_waveform(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    dataset = DCASE_task2_Dataset_test(paths, transform=transform)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=config['fit']['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# make path set and train/valid split\n",
    "############################################################################\n",
    "'''\n",
    "train_paths[machine_type]['train' or 'valid'] = path\n",
    "'''\n",
    "dev_train_paths = {}\n",
    "add_train_paths = {}\n",
    "train_paths = {}\n",
    "\n",
    "for machine_type in machine_types:\n",
    "    # dev train\n",
    "    dev_train_paths = [\"{}/{}/train/\".format(dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(dev_path, machine_type))]\n",
    "    dev_train_paths = sorted(dev_train_paths)\n",
    "    # add_dev train\n",
    "    add_train_paths = [\"{}/{}/train/\".format(add_dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(add_dev_path, machine_type))]\n",
    "    add_train_paths = sorted(add_train_paths)\n",
    "    # valid\n",
    "    dev_valid_paths = [\"{}/{}/test/\".format(dev_path, machine_type) + file for file in os.listdir(\"{}/{}/test\".format(dev_path, machine_type))]\n",
    "    dev_valid_paths = sorted(dev_valid_paths)\n",
    "    \n",
    "    train_paths[machine_type] = {}\n",
    "    train_paths[machine_type]['train'] = dev_train_paths + add_train_paths\n",
    "    train_paths[machine_type]['valid'] = dev_valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-30 18:12:25,129 - INFO - load_directory <- development\n",
      "2020-11-30 18:12:25,131 - INFO - ===========================\n",
      "2020-11-30 18:12:25,132 - INFO - [1/6] /media/hiroki/working/research/dcase2020/datasets/DCASE2/dev_data/ToyCar\n",
      "2020-11-30 18:12:25,133 - INFO - ============== MODEL LOAD ==============\n",
      "2020-11-30 18:12:26,107 - INFO - ============== CALC GMM PARAM : ToyCar ==============\n",
      "100%|██████████| 55/55 [00:05<00:00, 10.83it/s]\n",
      "2020-11-30 18:12:31,188 - INFO - target_dir : /media/hiroki/working/research/dcase2020/datasets/DCASE2/dev_data/ToyCar_id_01\n",
      "2020-11-30 18:12:31,195 - INFO - test_file  num : 601\n",
      "2020-11-30 18:12:31,196 - INFO - ============== BEGIN TEST FOR A MACHINE ID ==============\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 7000\n",
      "phi :\n",
      " tensor([3.3541e-05, 9.9992e-01, 4.3399e-05])\n",
      "mu :\n",
      " tensor([[ 0.0985, -0.0885,  0.0955, -0.1044, -0.1037,  0.2007],\n",
      "        [-0.0246,  0.0311, -0.0232,  0.0162,  0.0272,  0.0713],\n",
      "        [ 0.1417, -0.1353,  0.1362, -0.1435, -0.1439,  0.2440]])\n",
      "cov :\n",
      " tensor([[[ 0.0545, -0.0517,  0.0499, -0.0510, -0.0510,  0.0513],\n",
      "         [-0.0517,  0.0542, -0.0513,  0.0513,  0.0513, -0.0513],\n",
      "         [ 0.0499, -0.0513,  0.0548, -0.0503, -0.0523,  0.0514],\n",
      "         [-0.0510,  0.0513, -0.0503,  0.0553,  0.0513, -0.0514],\n",
      "         [-0.0510,  0.0513, -0.0523,  0.0513,  0.0549, -0.0514],\n",
      "         [ 0.0513, -0.0513,  0.0514, -0.0514, -0.0514,  0.0570]],\n",
      "\n",
      "        [[ 0.0065, -0.0035,  0.0019, -0.0027, -0.0029,  0.0016],\n",
      "         [-0.0035,  0.0057, -0.0031,  0.0029,  0.0030, -0.0015],\n",
      "         [ 0.0019, -0.0031,  0.0067, -0.0021, -0.0038,  0.0016],\n",
      "         [-0.0027,  0.0029, -0.0021,  0.0072,  0.0031, -0.0015],\n",
      "         [-0.0029,  0.0030, -0.0038,  0.0031,  0.0065, -0.0016],\n",
      "         [ 0.0016, -0.0015,  0.0016, -0.0015, -0.0016,  0.0018]],\n",
      "\n",
      "        [[ 0.0585, -0.0557,  0.0546, -0.0554, -0.0554,  0.0584],\n",
      "         [-0.0557,  0.0578, -0.0557,  0.0555,  0.0554, -0.0583],\n",
      "         [ 0.0546, -0.0557,  0.0588, -0.0547, -0.0564,  0.0585],\n",
      "         [-0.0554,  0.0555, -0.0547,  0.0595,  0.0555, -0.0584],\n",
      "         [-0.0554,  0.0554, -0.0564,  0.0555,  0.0588, -0.0585],\n",
      "         [ 0.0584, -0.0583,  0.0585, -0.0584, -0.0585,  0.0653]]])\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-48a10a6d1972>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0mnn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'z'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         \u001b[0msample_energy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_diag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_phi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_cov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_energy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hiroki/working/research/dcase2020/2D_codes/DPGMM_LSTM_CONV/strict_comp/latent5_mixture3/pytorch_model.py\u001b[0m in \u001b[0;36mcompute_energy\u001b[0;34m(self, z, phi, mu, cov, size_average)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mz_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mcov_inverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "#def run_eval(param, mode):\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# make output result directory\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# load base directory\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "\n",
    "# initialize lines in csv for AUC and pAUC\n",
    "csv_lines = []\n",
    "\n",
    "\n",
    "# loop of the base directory\n",
    "for idx, target_dir in enumerate(dirs):\n",
    "    com.logger.info(\"===========================\")\n",
    "    com.logger.info(\"[{idx}/{total}] {dirname}\".format(\n",
    "        dirname=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "    machine_type = os.path.split(target_dir)[1]\n",
    "\n",
    "    com.logger.info(\"============== MODEL LOAD ==============\")\n",
    "\n",
    "    model_file = \"{model}/{machine_type}_model.pth\".format(\n",
    "        model=param[\"model_directory\"],\n",
    "        machine_type=machine_type)\n",
    "\n",
    "    if not os.path.exists(model_file):\n",
    "        com.logger.error(\"{} model not found \".format(machine_type))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # define AE model\n",
    "    model = Model(sample_rate=config['preprocessing']['sample_rate'],\n",
    "                  window_size=config['preprocessing']['window_size'],\n",
    "                  hop_size=config['preprocessing']['hop_size'],\n",
    "                  mel_bins=config['preprocessing']['mel_bins'],\n",
    "                  fmin=config['preprocessing']['fmin'],\n",
    "                  fmax=config['preprocessing']['fmax'],\n",
    "                  latent_size=config['fit']['latent_size'],\n",
    "                  mixture_size=config['fit']['mixture_size']).to(device)\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    if mode:\n",
    "        # results by type\n",
    "        csv_lines.append([machine_type])\n",
    "        csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
    "        performance = []\n",
    "\n",
    "    machine_id_list = eval_func.get_machine_id_list_for_test(target_dir)\n",
    "    recons_outpath = RECONS_OUTDIR + '/' + machine_type\n",
    "    os.makedirs(recons_outpath, exist_ok=True)\n",
    "    \n",
    "    # calc train GMM param\n",
    "    com.logger.info(f\"============== CALC GMM PARAM : {machine_type} ==============\")\n",
    "    \n",
    "    train_loader = make_dataloader_test(train_paths[machine_type]['train'])\n",
    "    N = 0\n",
    "    mu_sum = 0\n",
    "    cov_sum = 0\n",
    "    gamma_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for sample in tqdm(train_loader):\n",
    "            input_data = sample['feature']\n",
    "            input_data = to_var(input_data)\n",
    "            nn_out = model(input_data, device)\n",
    "            tr_phi, tr_mu, tr_cov = model.compute_gmm_params(nn_out['z'], nn_out['gamma'])\n",
    "            tr_phi, tr_mu, tr_cov = tr_phi.to('cpu'), tr_mu.to('cpu'), tr_cov.to('cpu')\n",
    "            batch_gamma_sum = torch.sum(nn_out['gamma'].to('cpu'), dim=0).to('cpu')\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += tr_mu * batch_gamma_sum.unsqueeze(-1) # keep sums of the numerator only\n",
    "            cov_sum += tr_cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1) # keep sums of the numerator only\n",
    "            N += input_data.size(0)\n",
    "    train_phi = gamma_sum / N\n",
    "    train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "    train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "    print(\"N:\",N)\n",
    "    print(\"phi :\\n\",train_phi)\n",
    "    print(\"mu :\\n\",train_mu)\n",
    "    print(\"cov :\\n\",train_cov)\n",
    "    \n",
    "    # evaluation\n",
    "    for id_str in machine_id_list:\n",
    "\n",
    "        # load list of test files\n",
    "        test_files, y_true = eval_func.test_file_list_generator(target_dir, id_str, mode)\n",
    "\n",
    "        # setup anomaly score file path\n",
    "        anomaly_score_csv = \\\n",
    "            \"{result}/anomaly_score_{machine_type}_{id_str}.csv\"\\\n",
    "            .format(result=param[\"result_directory\"],\n",
    "                    machine_type=machine_type,\n",
    "                    id_str=id_str)\n",
    "        anomaly_score_list = []\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============== BEGIN TEST FOR A MACHINE ID ==============\")\n",
    "\n",
    "        y_pred = []\n",
    "        anomaly_count = 0\n",
    "        normal_count = 0\n",
    "        \n",
    "        test_loader = make_dataloader_test(test_files)\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        slicing = None\n",
    "        with torch.no_grad():\n",
    "            for it, data in enumerate(tqdm(test_loader)):\n",
    "                try:\n",
    "                    feature = data['feature']\n",
    "                    feature = to_var(feature)\n",
    "                    label = data['label']\n",
    "                    file_path = data['wav_name']\n",
    "                    # reconstruction through auto encoder in pytorch\n",
    "                    with torch.no_grad():\n",
    "                        nn_out = model(feature)\n",
    "                        z, _ = nn_out['z'], nn_out['gamma']\n",
    "                        sample_energy, cov_diag = model.compute_energy(z, phi=to_var(train_phi), mu=to_var(train_mu), cov=to_var(train_cov), size_average=False)\n",
    "                        preds = sample_energy.data.cpu().numpy()\n",
    "                        if it == 0:\n",
    "                            y_pred = preds.copy()\n",
    "                        else:\n",
    "                            y_pred = np.concatenate([y_pred, preds], axis=0)\n",
    "\n",
    "                    for idx in range(len(file_path)):\n",
    "                        anomaly_score_list.append([os.path.basename(file_path[idx]), preds[idx]])\n",
    "                except FileNotFoundError:\n",
    "                    com.logger.error(\"file broken!!\")\n",
    "\n",
    "        # save anomaly score\n",
    "        eval_func.save_csv(save_file_path=anomaly_score_csv,\n",
    "                           save_data=anomaly_score_list)\n",
    "        com.logger.info(\n",
    "            \"anomaly score result ->  {}\".format(anomaly_score_csv))\n",
    "\n",
    "        if mode:\n",
    "            # append AUC and pAUC to lists\n",
    "            auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "            p_auc = metrics.roc_auc_score(\n",
    "                y_true, y_pred, max_fpr=config[\"etc\"][\"max_fpr\"])\n",
    "            csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
    "            performance.append([auc, p_auc])\n",
    "            com.logger.info(\"AUC : {}\".format(auc))\n",
    "            com.logger.info(\"pAUC : {}\".format(p_auc))\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============ END OF TEST FOR A MACHINE ID ============\")\n",
    "\n",
    "    if mode:\n",
    "        # calculate averages for AUCs and pAUCs\n",
    "        averaged_performance = numpy.mean(\n",
    "            numpy.array(performance, dtype=float), axis=0)\n",
    "        csv_lines.append([\"Average\"] + list(averaged_performance))\n",
    "        csv_lines.append([])\n",
    "\n",
    "if mode:\n",
    "    # output results\n",
    "    result_path = \"{result}/{file_name}\".format(\n",
    "        result=param[\"result_directory\"],\n",
    "        file_name=param[\"result_file\"])\n",
    "    com.logger.info(\"AUC and pAUC results -> {}\".format(result_path))\n",
    "    eval_func.save_csv(save_file_path=result_path, save_data=csv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
