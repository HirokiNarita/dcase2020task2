{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# show reconstruct image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiroki/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# load config\n",
    "########################################################################\n",
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting I/O path\n",
    "########################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "eval_path = INPUT_ROOT + \"/eval_test\"\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "RESULT_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/result'\n",
    "RECONS_OUTDIR = OUTPUT_ROOT +'/eval_reconstruct_img'\n",
    "PKL_DIR = OUTPUT_ROOT +'/pkl'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default python-library\n",
    "########################################################################\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional python-library\n",
    "########################################################################\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import common as com\n",
    "import pytorch_modeler as modeler\n",
    "from pytorch_model import DAGMM as Model\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "yaml.warnings({'YAMLLoadWarning': False})\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "########################################################################\n",
    "import eval_functions as eval_func\n",
    "from pytorch_utils import to_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hiroki/anaconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "########################################################################\n",
    "# load config\n",
    "########################################################################\n",
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting seed\n",
    "########################################################################\n",
    "modeler.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting I/O path\n",
    "########################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "eval_path = INPUT_ROOT + \"/eval_test\"\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "RESULT_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/result'\n",
    "RECONS_OUTDIR = OUTPUT_ROOT +'/eval_reconstruct_img'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# for original function\n",
    "########################################################################\n",
    "param = {}\n",
    "param[\"dev_directory\"] = dev_path\n",
    "param[\"eval_directory\"] = eval_path\n",
    "param[\"model_directory\"] = MODEL_DIR\n",
    "param[\"result_directory\"] = RESULT_DIR\n",
    "param[\"result_file\"] = 'result.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def calc_time_anomaly(x, y, label, file_name):\n",
    "    fig = plt.figure(figsize=(10,5)) # width, height\n",
    "    fig.suptitle('label={}'.format(int(label)))\n",
    "    time_anomaly = np.zeros((x.shape[0]))\n",
    "    for frame in range(x.shape[0]):\n",
    "        time_anomaly[frame] = mse(y[frame,:],x[frame,:])\n",
    "    plt.plot(time_anomaly)\n",
    "    plt.title(f'label:{label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_anomaly.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(time_anomaly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(np.abs(x-y), aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reconstruct_img(x, y, label, file_name):\n",
    "    fig = plt.figure(figsize=(10,5)) # width, height\n",
    "    fig.suptitle('label={}'.format(int(label)))\n",
    "    ax1 = fig.add_subplot(121, title='x') # 明示的にAxesを作成する\n",
    "    sns.heatmap(x.T, ax=ax1) # ax1を参照するようにする\n",
    "    ax2 = fig.add_subplot(122, title='y')\n",
    "    sns.heatmap(y.T, ax=ax2)\n",
    "    fig.savefig('{}.png'.format(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-a3804855e7bb>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-a3804855e7bb>\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    num_workers=2,\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import preprocessing as prep\n",
    "\n",
    "class extract_waveform(object):\n",
    "    \"\"\"\n",
    "    wavデータロード(波形)\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    sound_data : waveform\n",
    "    \"\"\"\n",
    "    def __init__(self, sound_data=None):\n",
    "        self.sound_data = sound_data\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        self.sound_data = com.file_load(sample['wav_name'],\n",
    "                                        sr=config['preprocessing']['sample_rate'],\n",
    "                                        mono=config['preprocessing']['mono'])\n",
    "        self.sound_data = self.sound_data[0]\n",
    "        self.label = np.array(sample['label'])\n",
    "        self.wav_name = sample['wav_name']\n",
    "        \n",
    "        return {'feature': self.sound_data, 'label': self.label, 'wav_name': self.wav_name}\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"\n",
    "    Convert ndarrays in sample to Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        feature, label, wav_name = sample['feature'], sample['label'], sample['wav_name']\n",
    "        \n",
    "        return {'feature': torch.from_numpy(feature).float(), 'label': torch.from_numpy(label), 'wav_name': wav_name}\n",
    "\n",
    "class DCASE_task2_Dataset_test(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Attribute\n",
    "    ----------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, file_list, transform=None):\n",
    "        self.transform = transform\n",
    "        self.file_list = file_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_list[idx]\n",
    "        # ファイル名でlabelを判断\n",
    "        if \"normal\" in file_path:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "        \n",
    "        sample = {'wav_name':file_path, 'label':np.array(label)}\n",
    "        sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def make_dataloader_train(paths):\n",
    "    transform = transforms.Compose([\n",
    "        extract_waveform(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    dataset = DCASE_task2_Dataset_test(paths, transform=transform)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=config['fit']['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return test_loader\n",
    "    \n",
    "def make_dataloader_test(paths):\n",
    "    transform = transforms.Compose([\n",
    "        extract_waveform(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    dataset = DCASE_task2_Dataset_test(paths, transform=transform)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=config['fit']['batch_size'],\n",
    "        shuffle=True\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "        )\n",
    "    \n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# make path set and train/valid split\n",
    "############################################################################\n",
    "'''\n",
    "train_paths[machine_type]['train' or 'valid'] = path\n",
    "'''\n",
    "dev_train_paths = {}\n",
    "add_train_paths = {}\n",
    "train_paths = {}\n",
    "\n",
    "for machine_type in machine_types:\n",
    "    # dev train\n",
    "    dev_train_paths = [\"{}/{}/train/\".format(dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(dev_path, machine_type))]\n",
    "    dev_train_paths = sorted(dev_train_paths)\n",
    "    # add_dev train\n",
    "    add_train_paths = [\"{}/{}/train/\".format(add_dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(add_dev_path, machine_type))]\n",
    "    add_train_paths = sorted(add_train_paths)\n",
    "    # valid\n",
    "    dev_valid_paths = [\"{}/{}/test/\".format(dev_path, machine_type) + file for file in os.listdir(\"{}/{}/test\".format(dev_path, machine_type))]\n",
    "    dev_valid_paths = sorted(dev_valid_paths)\n",
    "    \n",
    "    train_paths[machine_type] = {}\n",
    "    train_paths[machine_type]['train'] = dev_train_paths + add_train_paths\n",
    "    train_paths[machine_type]['valid'] = dev_valid_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_base_path = '/media/hiroki/working/research/dcase2020/result/2D/DAGMM/strict_comp/latent5_mixture3_ver3/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.path.split(target_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def run_eval(param, mode):\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# make output result directory\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# load base directory\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "\n",
    "# initialize lines in csv for AUC and pAUC\n",
    "csv_lines = []\n",
    "\n",
    "\n",
    "# loop of the base directory\n",
    "for idx, target_dir in enumerate(dirs):\n",
    "    com.logger.info(\"===========================\")\n",
    "    com.logger.info(\"[{idx}/{total}] {dirname}\".format(\n",
    "        dirname=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "    machine_type = os.path.split(target_dir)[1]\n",
    "\n",
    "    com.logger.info(\"============== MODEL LOAD ==============\")\n",
    "\n",
    "    model_file = \"{model}/{machine_type}_model.pth\".format(\n",
    "        model=param[\"model_directory\"],\n",
    "        machine_type=machine_type)\n",
    "\n",
    "    if not os.path.exists(model_file):\n",
    "        com.logger.error(\"{} model not found \".format(machine_type))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # define AE model\n",
    "    model = Model(sample_rate=config['preprocessing']['sample_rate'],\n",
    "                  window_size=config['preprocessing']['window_size'],\n",
    "                  hop_size=config['preprocessing']['hop_size'],\n",
    "                  mel_bins=config['preprocessing']['mel_bins'],\n",
    "                  fmin=config['preprocessing']['fmin'],\n",
    "                  fmax=config['preprocessing']['fmax'],\n",
    "                  latent_size=config['fit']['latent_size'],\n",
    "                  mixture_size=config['fit']['mixture_size']).to(device)\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    if mode:\n",
    "        # results by type\n",
    "        csv_lines.append([machine_type])\n",
    "        csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
    "        performance = []\n",
    "\n",
    "    machine_id_list = eval_func.get_machine_id_list_for_test(target_dir)\n",
    "    recons_outpath = RECONS_OUTDIR + '/' + machine_type\n",
    "    os.makedirs(recons_outpath, exist_ok=True)\n",
    "    \n",
    "    # calc train GMM param\n",
    "    com.logger.info(f\"============== CALC GMM PARAM : {machine_type} ==============\")\n",
    "    \n",
    "    #train_loader = make_dataloader_test(train_paths[machine_type]['train'])\n",
    "    gmm_path = gmm_base_path + f'/{machine_type}_gmm_param.pkl'\n",
    "    gmm_param = pd.read_pickle(gmm_path)\n",
    "    # evaluation\n",
    "    for id_str in machine_id_list:\n",
    "\n",
    "        # load list of test files\n",
    "        test_files, y_true = eval_func.test_file_list_generator(target_dir, id_str, mode)\n",
    "\n",
    "        # setup anomaly score file path\n",
    "        anomaly_score_csv = \\\n",
    "            \"{result}/anomaly_score_{machine_type}_{id_str}.csv\"\\\n",
    "            .format(result=param[\"result_directory\"],\n",
    "                    machine_type=machine_type,\n",
    "                    id_str=id_str)\n",
    "        anomaly_score_list = []\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============== BEGIN TEST FOR A MACHINE ID ==============\")\n",
    "\n",
    "        y_pred = []\n",
    "        anomaly_count = 0\n",
    "        normal_count = 0\n",
    "        \n",
    "        test_loader = make_dataloader_test(test_files)\n",
    "        start_idx = 0\n",
    "        end_idx = 0\n",
    "        slicing = None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for it, data in enumerate(tqdm(test_loader)):\n",
    "                try:\n",
    "                    feature = data['feature']\n",
    "                    feature = to_var(feature)\n",
    "                    label = data['label']\n",
    "                    file_path = data['wav_name']\n",
    "                    # reconstruction through auto encoder in pytorch\n",
    "                    with torch.no_grad():\n",
    "                        nn_out = model(feature)\n",
    "                        z, _ = nn_out['z'], nn_out['gamma']\n",
    "                        sample_energy, cov_diag = model.compute_energy(z, phi=gmm_param[0], mu=gmm_param[1], cov=gmm_param[2], size_average=False)\n",
    "                        preds = sample_energy.data.cpu().numpy()\n",
    "                        if it == 0:\n",
    "                            y_pred = preds.copy()\n",
    "                        else:\n",
    "                            y_pred = np.concatenate([y_pred, preds], axis=0)\n",
    "\n",
    "                    for idx in range(len(file_path)):\n",
    "                        anomaly_score_list.append([os.path.basename(file_path[idx]), preds[idx]])\n",
    "                except FileNotFoundError:\n",
    "                    com.logger.error(\"file broken!!\")\n",
    "\n",
    "        # save anomaly score\n",
    "        eval_func.save_csv(save_file_path=anomaly_score_csv,\n",
    "                           save_data=anomaly_score_list)\n",
    "        com.logger.info(\n",
    "            \"anomaly score result ->  {}\".format(anomaly_score_csv))\n",
    "\n",
    "        if mode:\n",
    "            # append AUC and pAUC to lists\n",
    "            auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "            p_auc = metrics.roc_auc_score(\n",
    "                y_true, y_pred, max_fpr=config[\"etc\"][\"max_fpr\"])\n",
    "            csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
    "            performance.append([auc, p_auc])\n",
    "            com.logger.info(\"AUC : {}\".format(auc))\n",
    "            com.logger.info(\"pAUC : {}\".format(p_auc))\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============ END OF TEST FOR A MACHINE ID ============\")\n",
    "\n",
    "    if mode:\n",
    "        # calculate averages for AUCs and pAUCs\n",
    "        averaged_performance = numpy.mean(\n",
    "            numpy.array(performance, dtype=float), axis=0)\n",
    "        csv_lines.append([\"Average\"] + list(averaged_performance))\n",
    "        csv_lines.append([])\n",
    "\n",
    "if mode:\n",
    "    # output results\n",
    "    result_path = \"{result}/{file_name}\".format(\n",
    "        result=param[\"result_directory\"],\n",
    "        file_name=param[\"result_file\"])\n",
    "    com.logger.info(\"AUC and pAUC results -> {}\".format(result_path))\n",
    "    eval_func.save_csv(save_file_path=result_path, save_data=csv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
