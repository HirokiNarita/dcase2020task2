{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python default library\n",
    "import os\n",
    "import shutil\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# general analysis tool-kit\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# etc\n",
    "import yaml\n",
    "yaml.warnings({'YAMLLoadWarning': False})\n",
    "import mlflow\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# original library\n",
    "\n",
    "import common as com\n",
    "import pytorch_modeler as modeler\n",
    "from pytorch_model import CNN14AutoEncoder\n",
    "sys.path.append('./PANNs_utils')\n",
    "from PANNs_utils import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "panns_cnn14_path = '/media/hiroki/working/research/dcase2020/pre-trained_model/Cnn14_16k_mAP=0.438.pth'\n",
    "param = torch.load(panns_cnn14_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spectrogram_extractor.stft.conv_real.weight torch.Size([257, 1, 512])\n",
      "spectrogram_extractor.stft.conv_imag.weight torch.Size([257, 1, 512])\n",
      "logmel_extractor.melW torch.Size([257, 64])\n",
      "bn0.weight torch.Size([64])\n",
      "bn0.bias torch.Size([64])\n",
      "bn0.running_mean torch.Size([64])\n",
      "bn0.running_var torch.Size([64])\n",
      "bn0.num_batches_tracked torch.Size([])\n",
      "conv_block1.conv1.weight torch.Size([64, 1, 3, 3])\n",
      "conv_block1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "conv_block1.bn1.weight torch.Size([64])\n",
      "conv_block1.bn1.bias torch.Size([64])\n",
      "conv_block1.bn1.running_mean torch.Size([64])\n",
      "conv_block1.bn1.running_var torch.Size([64])\n",
      "conv_block1.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block1.bn2.weight torch.Size([64])\n",
      "conv_block1.bn2.bias torch.Size([64])\n",
      "conv_block1.bn2.running_mean torch.Size([64])\n",
      "conv_block1.bn2.running_var torch.Size([64])\n",
      "conv_block1.bn2.num_batches_tracked torch.Size([])\n",
      "conv_block2.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "conv_block2.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "conv_block2.bn1.weight torch.Size([128])\n",
      "conv_block2.bn1.bias torch.Size([128])\n",
      "conv_block2.bn1.running_mean torch.Size([128])\n",
      "conv_block2.bn1.running_var torch.Size([128])\n",
      "conv_block2.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block2.bn2.weight torch.Size([128])\n",
      "conv_block2.bn2.bias torch.Size([128])\n",
      "conv_block2.bn2.running_mean torch.Size([128])\n",
      "conv_block2.bn2.running_var torch.Size([128])\n",
      "conv_block2.bn2.num_batches_tracked torch.Size([])\n",
      "conv_block3.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "conv_block3.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "conv_block3.bn1.weight torch.Size([256])\n",
      "conv_block3.bn1.bias torch.Size([256])\n",
      "conv_block3.bn1.running_mean torch.Size([256])\n",
      "conv_block3.bn1.running_var torch.Size([256])\n",
      "conv_block3.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block3.bn2.weight torch.Size([256])\n",
      "conv_block3.bn2.bias torch.Size([256])\n",
      "conv_block3.bn2.running_mean torch.Size([256])\n",
      "conv_block3.bn2.running_var torch.Size([256])\n",
      "conv_block3.bn2.num_batches_tracked torch.Size([])\n",
      "conv_block4.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "conv_block4.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "conv_block4.bn1.weight torch.Size([512])\n",
      "conv_block4.bn1.bias torch.Size([512])\n",
      "conv_block4.bn1.running_mean torch.Size([512])\n",
      "conv_block4.bn1.running_var torch.Size([512])\n",
      "conv_block4.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block4.bn2.weight torch.Size([512])\n",
      "conv_block4.bn2.bias torch.Size([512])\n",
      "conv_block4.bn2.running_mean torch.Size([512])\n",
      "conv_block4.bn2.running_var torch.Size([512])\n",
      "conv_block4.bn2.num_batches_tracked torch.Size([])\n",
      "conv_block5.conv1.weight torch.Size([1024, 512, 3, 3])\n",
      "conv_block5.conv2.weight torch.Size([1024, 1024, 3, 3])\n",
      "conv_block5.bn1.weight torch.Size([1024])\n",
      "conv_block5.bn1.bias torch.Size([1024])\n",
      "conv_block5.bn1.running_mean torch.Size([1024])\n",
      "conv_block5.bn1.running_var torch.Size([1024])\n",
      "conv_block5.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block5.bn2.weight torch.Size([1024])\n",
      "conv_block5.bn2.bias torch.Size([1024])\n",
      "conv_block5.bn2.running_mean torch.Size([1024])\n",
      "conv_block5.bn2.running_var torch.Size([1024])\n",
      "conv_block5.bn2.num_batches_tracked torch.Size([])\n",
      "conv_block6.conv1.weight torch.Size([2048, 1024, 3, 3])\n",
      "conv_block6.conv2.weight torch.Size([2048, 2048, 3, 3])\n",
      "conv_block6.bn1.weight torch.Size([2048])\n",
      "conv_block6.bn1.bias torch.Size([2048])\n",
      "conv_block6.bn1.running_mean torch.Size([2048])\n",
      "conv_block6.bn1.running_var torch.Size([2048])\n",
      "conv_block6.bn1.num_batches_tracked torch.Size([])\n",
      "conv_block6.bn2.weight torch.Size([2048])\n",
      "conv_block6.bn2.bias torch.Size([2048])\n",
      "conv_block6.bn2.running_mean torch.Size([2048])\n",
      "conv_block6.bn2.running_var torch.Size([2048])\n",
      "conv_block6.bn2.num_batches_tracked torch.Size([])\n",
      "fc1.weight torch.Size([2048, 2048])\n",
      "fc1.bias torch.Size([2048])\n",
      "fc_audioset.weight torch.Size([527, 2048])\n",
      "fc_audioset.bias torch.Size([527])\n"
     ]
    }
   ],
   "source": [
    "for key in param['model'].keys():\n",
    "    print(key, param['model'][key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load config and set logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)\n",
    "\n",
    "log_folder = config['IO_OPTION']['OUTPUT_ROOT']+'/{0}.log'.format(datetime.date.today())\n",
    "logger = com.setup_logger(log_folder, '00_train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting seed\n",
    "modeler.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/hiroki/working/research/dcase2020/result/no_add_batch_16/config.yaml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################################################################\n",
    "# Setting I/O path\n",
    "############################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "TB_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/tb'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(TB_DIR, exist_ok=True)\n",
    "\n",
    "# copy config\n",
    "shutil.copy('./config.yaml', OUTPUT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make path list and train/valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "# make path set and train/valid split\n",
    "############################################################################\n",
    "'''\n",
    "train_paths[machine_type]['train' or 'valid'] = path\n",
    "'''\n",
    "dev_train_paths = {}\n",
    "add_train_paths = {}\n",
    "train_paths = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for machine_type in machine_types:\n",
    "    # dev train\n",
    "    dev_train_all_paths = [\"{}/{}/train/\".format(dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(dev_path, machine_type))]\n",
    "    dev_train_all_paths = sorted(dev_train_all_paths)\n",
    "    dev_train_paths[machine_type] = {}\n",
    "    dev_train_paths[machine_type]['train'], \\\n",
    "    dev_train_paths[machine_type]['valid'] = train_test_split(dev_train_all_paths,\n",
    "                                                              test_size=config['etc']['test_size'],\n",
    "                                                              shuffle=False,\n",
    "                                                             )\n",
    "    # add_dev train\n",
    "    add_train_all_paths = [\"{}/{}/train/\".format(add_dev_path, machine_type) + file for file in os.listdir(\"{}/{}/train\".format(add_dev_path, machine_type))]\n",
    "    add_train_all_paths = sorted(add_train_all_paths)\n",
    "    add_train_paths[machine_type] = {}\n",
    "    add_train_paths[machine_type]['train'], \\\n",
    "    add_train_paths[machine_type]['valid'] = train_test_split(add_train_all_paths,\n",
    "                                                              test_size=config['etc']['test_size'],\n",
    "                                                              shuffle=False,\n",
    "                                                             )\n",
    "    train_paths[machine_type] = {}\n",
    "    train_paths[machine_type]['train'] = dev_train_paths[machine_type]['train'] + add_train_paths[machine_type]['train']\n",
    "    train_paths[machine_type]['valid'] = dev_train_paths[machine_type]['valid'] + add_train_paths[machine_type]['valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# run\n",
    "#############################################################################\n",
    "def run(machine_type):\n",
    "    com.tic()\n",
    "    logger.info('TARGET MACHINE_TYPE: {0}'.format(machine_type))\n",
    "    logger.info('MAKE DATA_LOADER')\n",
    "    # dev_train_paths\n",
    "    dataloaders_dict = modeler.make_dataloader_array(train_paths, machine_type)\n",
    "    # define writer for tensorbord\n",
    "    os.makedirs(TB_DIR+'/'+machine_type, exist_ok=True)         # debug\n",
    "    tb_log_dir = TB_DIR + '/' + machine_type\n",
    "    writer = SummaryWriter(log_dir = tb_log_dir)\n",
    "    logger.info('TRAINING')\n",
    "    # parameter setting\n",
    "    net = CNN14AutoEncoder()\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    num_epochs = config['fit']['num_epochs']\n",
    "    history = modeler.train_net(net, dataloaders_dict, criterion, optimizer, num_epochs, writer)\n",
    "    # output\n",
    "    model = history['model']\n",
    "    model_out_path = MODEL_DIR+'/{}_model.pth'.format(machine_type)\n",
    "    torch.save(model.state_dict(), model_out_path)\n",
    "    logger.info('\\n success:{0} \\n'.format(machine_type) + \\\n",
    "                    'model_out_path ==> \\n {0}'.format(model_out_path))\n",
    "    #  close writer for tensorbord\n",
    "    writer.close()\n",
    "    #modeler.mlflow_log(history, config, machine_type, model_out_path, tb_log_dir)\n",
    "    com.toc()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# import default python-library\n",
    "########################################################################\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "import itertools\n",
    "import sys\n",
    "########################################################################\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# import additional python-library\n",
    "########################################################################\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "import common as com\n",
    "import pytorch_modeler as modeler\n",
    "from pytorch_model import VAE as ae\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "yaml.warnings({'YAMLLoadWarning': False})\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# load config\n",
    "########################################################################\n",
    "with open(\"./config.yaml\", 'rb') as f:\n",
    "    config = yaml.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting seed\n",
    "########################################################################\n",
    "modeler.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Setting I/O path\n",
    "########################################################################\n",
    "# input dirs\n",
    "INPUT_ROOT = config['IO_OPTION']['INPUT_ROOT']\n",
    "dev_path = INPUT_ROOT + \"/dev_data\"\n",
    "add_dev_path = INPUT_ROOT + \"/add_dev_data\"\n",
    "eval_path = INPUT_ROOT + \"/eval_test\"\n",
    "MODEL_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/models'\n",
    "# machine type\n",
    "MACHINE_TYPE = config['IO_OPTION']['MACHINE_TYPE']\n",
    "machine_types = os.listdir(dev_path)\n",
    "# output dirs\n",
    "OUTPUT_ROOT = config['IO_OPTION']['OUTPUT_ROOT']\n",
    "RESULT_DIR = config['IO_OPTION']['OUTPUT_ROOT'] + '/result'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# for original function\n",
    "########################################################################\n",
    "param = {}\n",
    "param[\"dev_directory\"] = dev_path\n",
    "param[\"eval_directory\"] = eval_path\n",
    "param[\"model_directory\"] = MODEL_DIR\n",
    "param[\"result_directory\"] = RESULT_DIR\n",
    "param[\"result_file\"] = 'result.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# def\n",
    "########################################################################\n",
    "def save_csv(save_file_path,\n",
    "             save_data):\n",
    "    with open(save_file_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, lineterminator='\\n')\n",
    "        writer.writerows(save_data)\n",
    "\n",
    "\n",
    "def get_machine_id_list_for_test(target_dir,\n",
    "                                 dir_name=\"test\",\n",
    "                                 ext=\"wav\"):\n",
    "    \"\"\"\n",
    "    target_dir : str\n",
    "        base directory path of \"dev_data\" or \"eval_data\"\n",
    "    test_dir_name : str (default=\"test\")\n",
    "        directory containing test data\n",
    "    ext : str (default=\"wav)\n",
    "        file extension of audio files\n",
    "\n",
    "    return :\n",
    "        machine_id_list : list [ str ]\n",
    "            list of machine IDs extracted from the names of test files\n",
    "    \"\"\"\n",
    "    # create test files\n",
    "    dir_path = os.path.abspath(\"{dir}/{dir_name}/*.{ext}\".format(dir=target_dir, dir_name=dir_name, ext=ext))\n",
    "    file_paths = sorted(glob.glob(dir_path))\n",
    "    # extract id\n",
    "    machine_id_list = sorted(list(set(itertools.chain.from_iterable(\n",
    "        [re.findall('id_[0-9][0-9]', ext_id) for ext_id in file_paths]))))\n",
    "    return machine_id_list\n",
    "\n",
    "\n",
    "def test_file_list_generator(target_dir,\n",
    "                             id_name,\n",
    "                             dir_name=\"test\",\n",
    "                             prefix_normal=\"normal\",\n",
    "                             prefix_anomaly=\"anomaly\",\n",
    "                             ext=\"wav\"):\n",
    "    \"\"\"\n",
    "    target_dir : str\n",
    "        base directory path of the dev_data or eval_data\n",
    "    id_name : str\n",
    "        id of wav file in <<test_dir_name>> directory\n",
    "    dir_name : str (default=\"test\")\n",
    "        directory containing test data\n",
    "    prefix_normal : str (default=\"normal\")\n",
    "        normal directory name\n",
    "    prefix_anomaly : str (default=\"anomaly\")\n",
    "        anomaly directory name\n",
    "    ext : str (default=\"wav\")\n",
    "        file extension of audio files\n",
    "\n",
    "    return :\n",
    "        if the mode is \"development\":\n",
    "            test_files : list [ str ]\n",
    "                file list for test\n",
    "            test_labels : list [ boolean ]\n",
    "                label info. list for test\n",
    "                * normal/anomaly = 0/1\n",
    "        if the mode is \"evaluation\":\n",
    "            test_files : list [ str ]\n",
    "                file list for test\n",
    "    \"\"\"\n",
    "    com.logger.info(\"target_dir : {}\".format(target_dir+\"_\"+id_name))\n",
    "\n",
    "    # development\n",
    "    if mode:\n",
    "        normal_files = sorted(\n",
    "            glob.glob(\"{dir}/{dir_name}/{prefix_normal}_{id_name}*.{ext}\".format(dir=target_dir,\n",
    "                                                                                 dir_name=dir_name,\n",
    "                                                                                 prefix_normal=prefix_normal,\n",
    "                                                                                 id_name=id_name,\n",
    "                                                                                 ext=ext)))\n",
    "        normal_labels = numpy.zeros(len(normal_files))\n",
    "        anomaly_files = sorted(\n",
    "            glob.glob(\"{dir}/{dir_name}/{prefix_anomaly}_{id_name}*.{ext}\".format(dir=target_dir,\n",
    "                                                                                  dir_name=dir_name,\n",
    "                                                                                  prefix_anomaly=prefix_anomaly,\n",
    "                                                                                  id_name=id_name,\n",
    "                                                                                  ext=ext)))\n",
    "        anomaly_labels = numpy.ones(len(anomaly_files))\n",
    "        files = numpy.concatenate((normal_files, anomaly_files), axis=0)\n",
    "        labels = numpy.concatenate((normal_labels, anomaly_labels), axis=0)\n",
    "        com.logger.info(\"test_file  num : {num}\".format(num=len(files)))\n",
    "        if len(files) == 0:\n",
    "            com.logger.exception(\"no_wav_file!!\")\n",
    "        print(\"\\n========================================\")\n",
    "\n",
    "    # evaluation\n",
    "    else:\n",
    "        files = sorted(\n",
    "            glob.glob(\"{dir}/{dir_name}/*{id_name}*.{ext}\".format(dir=target_dir,\n",
    "                                                                  dir_name=dir_name,\n",
    "                                                                  id_name=id_name,\n",
    "                                                                  ext=ext)))\n",
    "        labels = None\n",
    "        com.logger.info(\"test_file  num : {num}\".format(num=len(files)))\n",
    "        if len(files) == 0:\n",
    "            com.logger.exception(\"no_wav_file!!\")\n",
    "        print(\"\\n=========================================\")\n",
    "\n",
    "    return files, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-22 19:40:16,241 - INFO - load_directory <- development\n",
      "2020-09-22 19:40:16,243 - INFO - ===========================\n",
      "2020-09-22 19:40:16,244 - INFO - [1/6] /media/hiroki/working/research/dcase2020/datasets/DCASE2/dev_data/ToyCar\n",
      "2020-09-22 19:40:16,245 - INFO - ============== MODEL LOAD ==============\n",
      "2020-09-22 19:40:16,283 - INFO - target_dir : /media/hiroki/working/research/dcase2020/datasets/DCASE2/dev_data/ToyCar_id_01\n",
      "2020-09-22 19:40:16,291 - INFO - test_file  num : 601\n",
      "2020-09-22 19:40:16,292 - INFO - ============== BEGIN TEST FOR A MACHINE ID ==============\n",
      "/home/hiroki/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1614: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "tensor(1064.9595, device='cuda:0')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c7e8fccd51ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     data, device=device, dtype=torch.float32)\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m#print(pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# make output result directory\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "\n",
    "# load base directory\n",
    "dirs = com.select_dirs(param=param, mode=mode)\n",
    "\n",
    "# initialize lines in csv for AUC and pAUC\n",
    "csv_lines = []\n",
    "\n",
    "# loop of the base directory\n",
    "for idx, target_dir in enumerate(dirs):\n",
    "    com.logger.info(\"===========================\")\n",
    "    com.logger.info(\"[{idx}/{total}] {dirname}\".format(\n",
    "        dirname=target_dir, idx=idx+1, total=len(dirs)))\n",
    "\n",
    "    machine_type = os.path.split(target_dir)[1]\n",
    "\n",
    "    com.logger.info(\"============== MODEL LOAD ==============\")\n",
    "\n",
    "    model_file = \"{model}/{machine_type}_model.pth\".format(\n",
    "        model=param[\"model_directory\"],\n",
    "        machine_type=machine_type)\n",
    "\n",
    "    if not os.path.exists(model_file):\n",
    "        com.logger.error(\"{} model not found \".format(machine_type))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # define AE model\n",
    "    model = ae().to(device)\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "\n",
    "    if mode:\n",
    "        # results by type\n",
    "        csv_lines.append([machine_type])\n",
    "        csv_lines.append([\"id\", \"AUC\", \"pAUC\"])\n",
    "        performance = []\n",
    "\n",
    "    machine_id_list = get_machine_id_list_for_test(target_dir)\n",
    "\n",
    "    for id_str in machine_id_list:\n",
    "\n",
    "        # load list of test files\n",
    "        test_files, y_true = test_file_list_generator(target_dir, id_str)\n",
    "\n",
    "        # setup anomaly score file path\n",
    "        anomaly_score_csv = \\\n",
    "            \"{result}/anomaly_score_{machine_type}_{id_str}.csv\"\\\n",
    "            .format(result=param[\"result_directory\"],\n",
    "                    machine_type=machine_type,\n",
    "                    id_str=id_str)\n",
    "        anomaly_score_list = []\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============== BEGIN TEST FOR A MACHINE ID ==============\")\n",
    "\n",
    "        y_pred = [0. for k in test_files]\n",
    "\n",
    "        for file_idx, file_path in enumerate(test_files):\n",
    "            try:\n",
    "                data = com.file_to_vector_array(\n",
    "                    file_path,\n",
    "                    n_mels=config[\"mel_spectrogram_param\"][\"n_mels\"],\n",
    "                    frames=config[\"mel_spectrogram_param\"][\"frames\"],\n",
    "                    n_fft=config[\"mel_spectrogram_param\"][\"n_fft\"],\n",
    "                    hop_length=config[\"mel_spectrogram_param\"][\"hop_length\"],\n",
    "                    power=config[\"mel_spectrogram_param\"][\"power\"])\n",
    "\n",
    "                # reconstruction through auto encoder in pytorch\n",
    "                feed_data = torch.as_tensor(\n",
    "                    data, device=device, dtype=torch.float32)\n",
    "                with torch.no_grad():\n",
    "                    _, _, pred = model(feed_data, device)\n",
    "                    pred = pred.to('cpu').detach().numpy().copy()\n",
    "                    #print(pred)\n",
    "\n",
    "                errors = numpy.mean(numpy.square(data - pred), axis=1)\n",
    "                y_pred[file_idx] = numpy.mean(errors)\n",
    "                anomaly_score_list.append(\n",
    "                    [os.path.basename(file_path), y_pred[file_idx]])\n",
    "            except FileNotFoundError:\n",
    "                com.logger.error(\"file broken!!: {}\".format(file_path))\n",
    "\n",
    "        # save anomaly score\n",
    "        save_csv(save_file_path=anomaly_score_csv,\n",
    "                    save_data=anomaly_score_list)\n",
    "        com.logger.info(\n",
    "            \"anomaly score result ->  {}\".format(anomaly_score_csv))\n",
    "\n",
    "        if mode:\n",
    "            # append AUC and pAUC to lists\n",
    "            auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "            p_auc = metrics.roc_auc_score(\n",
    "                y_true, y_pred, max_fpr=config[\"etc\"][\"max_fpr\"])\n",
    "            csv_lines.append([id_str.split(\"_\", 1)[1], auc, p_auc])\n",
    "            performance.append([auc, p_auc])\n",
    "            com.logger.info(\"AUC : {}\".format(auc))\n",
    "            com.logger.info(\"pAUC : {}\".format(p_auc))\n",
    "\n",
    "        com.logger.info(\n",
    "            \"============ END OF TEST FOR A MACHINE ID ============\")\n",
    "\n",
    "    if mode:\n",
    "        # calculate averages for AUCs and pAUCs\n",
    "        averaged_performance = numpy.mean(\n",
    "            numpy.array(performance, dtype=float), axis=0)\n",
    "        csv_lines.append([\"Average\"] + list(averaged_performance))\n",
    "        csv_lines.append([])\n",
    "\n",
    "if mode:\n",
    "    # output results\n",
    "    result_path = \"{result}/{file_name}\".format(\n",
    "        result=param[\"result_directory\"],\n",
    "        file_name=param[\"result_file\"])\n",
    "    com.logger.info(\"AUC and pAUC results -> {}\".format(result_path))\n",
    "    save_csv(save_file_path=result_path, save_data=csv_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
